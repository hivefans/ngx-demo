---
--- Generated by EmmyLua(https://github.com/EmmyLua)
--- Created by mac.
--- DateTime: 2018/5/24 下午7:20
---


local librdkafka = require 'rdkafka.librdkafka'
local KafkaConfig = require 'rdkafka.config'
local KafkaTopic = require 'rdkafka.topic'
local ffi = require 'ffi'

local DEFAULT_DESTROY_TIMEOUT_MS = 3000

local KafkaConsumer = {}
KafkaConsumer.__index = KafkaConsumer


--[[
    Creates a new Kafka consumer.

    'kafka_config' is an optional object that will be used instead of the default
    configuration.
    The 'kafka_config' object is reusable after this call.

    'destroy_timeout_ms' is a parameter that is used to determine how long client
    will wait while all rd_kafka_t objects will be destroyed.

    Returns the new object on success or "error(errstr)" on error in which case
    'errstr' is set to a human readable error message.
]]--

function KafkaConsumer.create(kafka_config, destroy_timeout_ms)
    local config = nil
    if kafka_config ~= nil then
        config = KafkaConfig.create(kafka_config).kafka_conf_
        ffi.gc(config, nil)
    end

    local ERRLEN = 256
    local errbuf = ffi.new("char[?]", ERRLEN) -- cdata objects are garbage collected
    local kafka = librdkafka.rd_kafka_new(librdkafka.RD_KAFKA_CONSUMER, config, errbuf, ERRLEN)

    if kafka == nil then
        error(ffi.string(errbuf))
    end

    local consumer = {kafka_ = kafka}
    KafkaTopic.kafka_topic_map_[kafka] = {}

    setmetatable(consumer, KafkaConsumer)
    ffi.gc(consumer.kafka_, function (...)
        for k, topic_ in pairs(KafkaTopic.kafka_topic_map_[consumer.kafka_]) do
            librdkafka.rd_kafka_topic_destroy(topic_)
        end
        KafkaTopic.kafka_topic_map_[consumer.kafka_] = nil
        librdkafka.rd_kafka_destroy(...)
        librdkafka.rd_kafka_wait_destroyed(destroy_timeout_ms or DEFAULT_DESTROY_TIMEOUT_MS)
    end
    )
    consumer.exit_flag = false
    return consumer
end


--[[
    Adds a one or more brokers to the kafka handle's list of initial brokers.
    Additional brokers will be discovered automatically as soon as rdkafka
    connects to a broker by querying the broker metadata.

    If a broker name resolves to multiple addresses (and possibly
    address families) all will be used for connection attempts in
    round-robin fashion.

    'broker_list' is a ,-separated list of brokers in the format:
    <host1>[:<port1>],<host2>[:<port2>]...

    Returns the number of brokers successfully added.

    NOTE: Brokers may also be defined with the 'metadata.broker.list'
    configuration property.
]]--

function KafkaConsumer:brokers_add(broker_list)
    assert(self.kafka_ ~= nil)
    return librdkafka.rd_kafka_brokers_add(self.kafka_, broker_list)
end


--[[
    Produce and send a single message to broker.

    `produce()` is an asynch non-blocking API.

    'partition' is the target partition, either:
     - RD_KAFKA_PARTITION_UA (unassigned) for
        automatic partitioning using the topic's partitioner function, or
     - a fixed partition (0..N)

    'payload' is the message payload.

    'key' is an optional message key, if non-nil it will be passed to the topic
    partitioner as well as be sent with the message to the broker and passed
    on to the consumer.


    Returns "error(errstr)" on error in which case 'errstr' is set to a human
    readable error message.
]]--
local function msg_consume(rkmessage)

    if rkmessage then
        if rkmessage.payload then
            ngx.log(ngx.ERR,'------',rkmessage.payload)
        end
    end

end


function KafkaConsumer:consume_start(kafka_topic, partition,_offset, _callback)
    assert(self.kafka_ ~= nil)
    assert(kafka_topic.topic_ ~= nil)


    --if (rd_kafka_consume_start(rkt, partition, start_offset) == -1){
    --    rd_kafka_resp_err_t err = rd_kafka_last_error();
    --    fprintf(stderr, "%% Failed to start consuming: %s\n",
    --            rd_kafka_err2str(err));
    --    if (err == RD_KAFKA_RESP_ERR__INVALID_ARG)
    --        fprintf(stderr,
    --                "%% Broker based offset storage "
    --                "requires a group.id, "
    --                "add: -X group.id=yourGroup\n");
    --        exit(1);
    --        }

    if not  _offset  then _offset = -1 end

    local rkt = kafka_topic.topic_
    if librdkafka.rd_kafka_consume_start(rkt, partition, _offset) == -1 then
        ngx.log(ngx.ERR,"consumer start error!")
        return nil
    end
    while not self.exit_flag do
        local rkmessage = librdkafka.rd_kafka_consume(rkt, partition, 1000);
        if rkmessage ~= 0 then


            librdkafka.rd_kafka_message_destroy(rkmessage);
        end

    end


    local RD_KAFKA_MSG_F_COPY = 0x2
    local produce_result = librdkafka.rd_kafka_produce(kafka_topic.topic_, partition, RD_KAFKA_MSG_F_COPY,
            ffi.cast("void*", payload), #payload, ffi.cast("void*", key), keylen, nil)

    if produce_result == -1 then
        error(ffi.string(librdkafka.rd_kafka_err2str(librdkafka.rd_kafka_errno2err(ffi.errno()))))
    end
end

--[[
    Polls the provided kafka handle for events.

    Events will cause application provided callbacks to be called.

    The 'timeout_ms' argument specifies the minimum amount of time
    (in milliseconds) that the call will block waiting for events.
    For non-blocking calls, provide 0 as 'timeout_ms'.
    To wait indefinately for an event, provide -1.

    Events:
    - delivery report callbacks  (if dr_cb is configured) [producer]
    - error callbacks (if error_cb is configured) [producer & consumer]
    - stats callbacks (if stats_cb is configured) [producer & consumer]

    Returns the number of events served.

    NOTE: This function doesn't use jit compilation
]]--

function KafkaConsumer:poll(timeout_ms)
    assert(self.kafka_ ~= nil)
    return librdkafka.rd_kafka_poll(self.kafka_, timeout_ms)
end
jit.off(KafkaConsumer.poll)

--[[
    Returns the current out queue length:
    messages waiting to be sent to, or acknowledged by, the broker.
]]--

function KafkaConsumer:outq_len()
    assert(self.kafka_ ~= nil)
    return librdkafka.rd_kafka_outq_len(self.kafka_)
end

--[[
    Retrieve the current number of threads in use by librdkafka.
]]--

function KafkaConsumer.thread_cnt()
    return librdkafka.rd_kafka_thread_cnt()
end

return KafkaConsumer
